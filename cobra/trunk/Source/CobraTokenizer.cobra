class CobraTokenizer
	inherits Tokenizer

	test
		# There are plenty of external Cobra source code tests that will exercise the lexer.
		# But here are a few basic tests to make sure the tokenizer has some viability.
		t = CobraTokenizer()
		# TODO:
		t.startSource('foo bar')
		tokens = t.allTokens
		Tokenizer.checkTokens(tokens, 'ID ID EOL')

		t.restart
		t.startSource('class Foo\n\tdef foo()\n\t\treturn 1')
#		t.startSource(
#'''class Foo
#	def foo()
#		return 1
#''')
		tokens = t.allTokens
		Tokenizer.checkTokens(tokens, 'CLASS ID EOL INDENT DEF OPEN_CALL RPAREN EOL INDENT RETURN INTEGER_LIT EOL DEDENT DEDENT')

		t.restart
		t.startSource('class Foo\n\tpass\n\nclass Bar\n\tpass')
#		t.startSource(
#	'''class Foo
#		pass
#
#	class Bar
#		pass
#	''')
		tokens = t.allTokens
		Tokenizer.checkTokens(tokens, 'CLASS ID EOL INDENT PASS EOL EOL DEDENT CLASS ID EOL INDENT PASS EOL DEDENT')

		t.restart
		t.startSource('class Foo\n\tdef foo()\n\t\treturn 1\n\nclass Bar\n\tpass\n')
#	t.startSource(
#'''class Foo
#	def foo()
#		return 1
#
#class Bar
#	pass
#''')
		tokens = t.allTokens
		Tokenizer.checkTokens(tokens, 'CLASS ID EOL INDENT DEF OPEN_CALL RPAREN EOL INDENT RETURN INTEGER_LIT EOL EOL DEDENT DEDENT CLASS ID EOL INDENT PASS EOL DEDENT')

	var _indentCount as int
	var _substLBracketCount as int
	var _inSubstStringSingle = false
	var _inSubstStringDouble = false
	var _inDocString = false

	def init
		base.init

	def init(verbosity as int)
		base.init
		_verbosity = verbosity

	def addInfo(sb as StringBuilder) is override
		base.addInfo(sb)
		sb.append('_indentCount=[_indentCount], ')
		sb.append('_substLBracketCount=[_substLBracketCount], ')
		sb.append('_inSubstStringSingle=[_inSubstStringSingle], ')
		sb.append('_inDocString=[_inDocString]')

	# Note: The Tokenizer class handles it's input one line at a time,
	#       and retains the \n at the end of the line. This affects
	#       the regex design for the tokens below.

	get orderedTokenSpecs as List<of String> is override
		return [
			# whitespace
			r'BLANK_TABS_LINE_1		^[\t]+$',
			r'BLANK_TABS_LINE_2		^[\t]+\#.*$',
			r'INDENT_MIXED_TSNS		^[\t]+[ ]+(?=[^\t ])',
			r'INDENT_MIXED_TS		^[\t]+[ ]+',
			r'INDENT_MIXED_ST		^[ ]+[\t]+',
			r'INDENT_ALL_TABS		^[\t]+',
			r'INDENT_ALL_SPACES		^[ ]+',
			r'NO_INDENT				^(?=[^\t\n#])',
			r'EOL					\n',
			r'SINGLE_LINE_COMMENT	\#.*',
			r'SPACE					[ \t]+',

			r'OPEN_GENERIC			[A-Za-z_][A-Za-z0-9_]*<of[ \n\r\t]',
			r'OPEN_IF				if\(',
			r'OPEN_CALL				[A-Za-z_][A-Za-z0-9_]*\(',

			r'FLOAT_LIT				\d[\d_]*\.\d+f',
			r'DECIMAL_LIT			\d[\d_]*\.\d+d?',
			r'INTEGER_LIT			\d[\d_]*',

			r'INT_SIZE				int[0-9]+',
			r'UINT_SIZE				uint[0-9]+',
			r'FLOAT_SIZE			float[0-9]+',

			r"CHAR_LIT_SINGLE		c'\\?.'",
			r'CHAR_LIT_DOUBLE		c"\\?."',

			# doc strings
			r'DOC_STRING_START		"""[ \t]*\n',

			# sharp strings
			r"SHARP_SINGLE			sharp'[^'\n]*'",
			r'SHARP_DOUBLE			sharp"[^"\n]*"',

			# raw strings
			r"STRING_RAW_SINGLE		r'[^'\n]*'",
			r'STRING_RAW_DOUBLE		r"[^"\n]*"',

			# substituted strings
			r'RBRACKET_SPECIAL		]',
			r"STRING_START_SINGLE	'[^'\n\[]*\[",
			r"STRING_PART_SINGLE	\][^'\n\[]*\[",
			r"STRING_STOP_SINGLE	\][^'\n\[]*'",

			r'STRING_START_DOUBLE	"[^"\n\[]*\[',
			r'STRING_PART_DOUBLE	\][^"\n\[]*\[',
			r'STRING_STOP_DOUBLE	\][^"\n\[]*"',

			r'STRING_PART_FORMAT	:[^X"\n\[]*(?=])'.replace('X', "'"),

			# plain strings
			r"STRING_NOSUB_SINGLE	ns'[^'\n]*'",
			r'STRING_NOSUB_DOUBLE	ns"[^"\n]*"',

			r"STRING_SINGLE			'[^'\n]*'",
			r'STRING_DOUBLE			"[^"\n]*"',

			r'TOQ					to\?',
			r'ID					[A-Za-z_][A-Za-z0-9_]*',
		]

	get unorderedTokenSpecs as List<of String> is override
		return [
			r'SHARP_OPEN		\$sharp\(',   # deprecated. $ is reserved for future language level regex support
			r"SINGLE_QUOTE		'",
			r'DOUBLE_QUOTE		"',
			r'DOT				\.',
			r'DOTDOT			\.\.',
			r'COLON				:',
			r'PLUS				\+',
			r'PLUSPLUS			\+\+',
			r'MINUSMINUS		\-\-',
			r'MINUS				-',
			r'STARSTAR			\*\*',
			r'STAR				\*',
			r'SLASHSLASH		//',
			r'SLASH				/',
			r'PERCENTPERCENT	%%',
			r'PERCENT			%',
			r'ASSIGN			=',
			r'LPAREN			\(',
			r'RPAREN			\)',
			r'LBRACKET			\[',
			r'RBRACKET			\]',
			r'LCURLY			\{',
			r'RCURLY			\}',
			r'SEMI				;',
			r'COMMA				,',
			r'DOUBLE_LT			<<',
			r'DOUBLE_GT			>>',
			r'DICT_OPEN			{',
			r'DICT_CLOSE		}',
			r'QUESTION			\?',
			r'BANG				\!',
			r'ARRAY_OPEN		\@\[',
			r'AMPERSAND			\&',
			r'VERTICAL_BAR		\|',
			r'CARET				\^',
			r'TILDE				\~',

			r'EQ				==',
			r'NE				<>',
			r'LT				<',
			r'GT				>',
			r'LE				<=',
			r'GE				>=',

			r'PLUS_EQUALS		\+=',
			r'MINUS_EQUALS		\-=',
			r'STAR_EQUALS		\*=',
			r'SLASH_EQUALS		\/=',
			r'PERCENT_EQUALS	%=',
			r'QUESTION_EQUALS	\?=',
			r'BANG_EQUALS		\!=',
		]

	get keywords as String is override
		# CC: this should be multiline string with comments
		return 'use import namespace enum class inherits implements interface struct extend where must be callable cue invariant def sig as get set pro var from has body test shared virtual override ref vari require ensure old this base is assert branch on off expect if else using while post for down to step break continue try catch success finally throw except event listen ignore raise and or not implies any every all to to\\? pass print stop trace return yield in bool char int decimal float of passthrough true false nil dynamic same instance where inlined end number global objc out inout'

	def _reset is override
		base._reset
		_indentCount = 0
		_substLBracketCount = 0

	def afterStart is override
		base.afterStart
		# CC:
		# _tokenDefsByWhich['STRING_PART_SINGLE'].isActiveCall = def(tokenizer)=tokenizer.inSubstStringSingle
		# _tokenDefsByWhich['STRING_STOP_SINGLE'].isActiveCall = def(tokenizer)=tokenizer.inSubstStringSingle
		for which in 'RBRACKET_SPECIAL STRING_PART_SINGLE STRING_STOP_SINGLE STRING_PART_DOUBLE STRING_STOP_DOUBLE STRING_PART_FORMAT'.split(c' ')
			_tokenDefsByWhich[which].isActive = false

	def isActiveCall(tok as TokenDef) as bool is override
		if tok.which=='STRING_PART_SINGLE' or tok.which=='STRING_STOP_SINGLE'
			return _inSubstStringSingle
		return true

	get _nextToken as IToken? is override
		# overridden to deliver the final DEDENTS to close out indentation
		tok = base._nextToken
		if tok is nil
			while _indentCount > 0
				t = Token(.lastToken.fileName, .lastToken.lineNum, 1, .lastToken.charNum, 'DEDENT', '', '')
				_tokenQueue.enqueue(t)
				_indentCount -= 1
			if _tokenQueue.count
				return _nextToken
			else
				return nil
		else
			return tok

	def onBLANK_TABS_LINE_1(tok as IToken) as IToken?
		# Eat these.
		# Don't muck with perceived indentation level as
		# these kinds of lines are irrelevant.
		#print '<> onBLANK_TABS_LINE_1'
		return nil

	def onBLANK_TABS_LINE_2(tok as IToken) as IToken?
		#print '<> onBLANK_TABS_LINE_2'
		return nil

	def onINDENT_MIXED_TSNS(tok as IToken) as IToken?
		# expecting tabs, spaces, non-whitespace
		assert tok.text.startsWith('\t')
		assert tok.text.endsWith(' ')
		# this is okay on continued lines
		if .justDidLineContinuation
			indentLevel = Utils.countChars(tok.text, c'\t') + Utils.countChars(tok.text, c' ') // 4
			return _processNumIndentLevels(indentLevel)  # will check continuation indentation rules
		else
			return .onINDENT_MIXED_TS(tok)

	def onINDENT_MIXED_TS(tok as IToken) as IToken?
		sb = StringBuilder()
		for c in tok.text
			if c==c' '
				sb.append(r'[SPACE]')
			else if c==c'\t'
				sb.append(r'[TAB]')
			else
				sb.append(c)
		.throwError('Cannot mix tabs and spaces in indentation. [sb]...')
		return nil  # make compiler happy.

	def onINDENT_MIXED_ST(tok as IToken) as IToken?
		return .onINDENT_MIXED_TS(tok)

	def onINDENT_ALL_TABS(tok as IToken) as IToken?
		numTabs = Utils.countChars(tok.text, c'\t')
		return _processNumIndentLevels(numTabs)

	def onINDENT_ALL_SPACES(tok as IToken) as IToken?
		numSpaces = Utils.countChars(tok.text, c' ')
		if numSpaces % 4 and not .justDidLineContinuation # yes, 4. hard coded, intentionally.
			# TODO: should really just record an error and take (numSpaces/4).round as the indent
			.throwError('Space-based indentation must be a multiple of 4. This line has a remainder of [numSpaces%4].')
		return _processNumIndentLevels(numSpaces // 4)

	def onNO_INDENT(tok as IToken) as IToken?
		require tok.text==''
		_curTokenDef.ignoreCount = 1
		t = _processNumIndentLevels(0)
		return t

	def _processNumIndentLevels(numTabs as int) as IToken?
		if .justDidLineContinuation
			if numTabs < _indentCount
				.recordError('Must indent same amount or more on a continued line.')
			return nil
		firstTok as IToken?
		lastTok as IToken?
		while numTabs > _indentCount
			_indentCount += 1
			newTok = Token(_fileName, _lineNum, 1, _charNum, 'INDENT', '', '')
			if lastTok
				lastTok.nextToken = newTok
				lastTok = newTok
			else
				firstTok = lastTok = newTok
		if firstTok
			return firstTok
		while numTabs < _indentCount
			_indentCount -= 1
			newTok = Token(_fileName, _lineNum, 1, _charNum, 'DEDENT', '', '')
			if lastTok
				lastTok.nextToken = newTok
				lastTok = newTok
			else
				firstTok = lastTok = newTok
		return firstTok

	var _didLineContinuation as bool  # only meaningful after an EOL

	get justDidLineContinuation as bool
		return .lastToken and .lastToken.which == 'EOL' and _didLineContinuation

	def onEOL(tok as IToken) as IToken?
		_didLineContinuation = .lastToken and .lastToken.text == '_' and .lastToken.which == 'ID'
		return tok

	def onSINGLE_LINE_COMMENT(tok as IToken) as IToken?
		# eat these
		return nil

	def onSPACE(tok as IToken) as IToken?
		# eat these
		return nil

	def onOPEN_CALL(tok as IToken) as IToken?
		tok.value = tok.text[:-1]
		return tok

	def onOPEN_GENERIC(tok as IToken) as IToken?
		require tok.text.trim.endsWith('<of')
		s = tok.text.trim
		tok.value = s[:-3]
		return tok

	def onID(tok as IToken) as IToken?
		tok = .keywordOrWhich(tok, 'ID')
		if tok.which<>'ID'
			tok.isKeyword = true
		return tok

	def onFLOAT_LIT(tok as IToken) as IToken?
		try
			s = tok.text.replace('_', '')
			if s.endsWith('f')
				s = s[:-1]
			tok.value = float.parse(s, Utils.cultureInfoForNumbers)
		catch FormatException
			assert false, 'not expecting to get here given regex'
		catch OverflowException
			assert false, 'TODO'
		return tok

	def onDECIMAL_LIT(tok as IToken) as IToken?
		try
			s = tok.text.replace('_', '')
			if s.endsWith('d')
				s = s[:-1]
			tok.value = decimal.parse(s, Utils.cultureInfoForNumbers)
		catch FormatException
			assert false, 'not expecting to get here given regex'
		catch OverflowException
			assert false, 'TODO'
		return tok

	def onINTEGER_LIT(tok as IToken) as IToken?
		try
			tok.value = int.parse(tok.text.replace('_', ''), Utils.cultureInfoForNumbers)
		catch FormatException
			assert false, 'not expecting to get here given regex'
		catch OverflowException
			assert false, 'TODO'
		return tok

	def onINT_SIZE(tok as IToken) as IToken?
		size = int.parse(tok.text[3:]) to int
		tok.value = size
		return tok

	def onUINT_SIZE(tok as IToken) as IToken?
		size = int.parse(tok.text[4:]) to int
		tok.value = size
		return tok

	def onFLOAT_SIZE(tok as IToken) as IToken?
		size = int.parse(tok.text[5:]) to int
		tok.value = size
		return tok
		# TODO: finish this

	def onCHAR_LIT_SINGLE(tok as IToken) as IToken?
		return _onCharLit(tok)

	def onCHAR_LIT_DOUBLE(tok as IToken) as IToken?
		return _onCharLit(tok)

	def _onCharLit(tok as IToken) as IToken?
		require tok.text.startsWith('c')
		s = tok.text[2:-1]
		assert s.length==1 or s.length==2
		tok.value = s
		return tok


	##
	## String substitution handling
	##

	def tokValueForString(s as String) as String
		"""
		Utility class for onSTRING_START|PART|STOP_SINGLE|DOUBLE.
		"""
		require
			s.length >= 2  # CC: #s[0] in [c'"', c"'"] #s[s.length-1] in [c'"', c"'"]
		body
			s = s.substring(1, s.length-2)
			chars = StringBuilder()  # TODO: make an array of chars
			last = c'\0'
			next as char?
			for c in s
				next = nil
				if last==c'\\'
					branch c
						# TODO: any other codes?
						on c'n': next = c'\n'
						on c'r': next = c'\r'
						on c't': next = c'\t'
						on c'0': next = c'\0'
						on c'\\'
							chars.append(c'\\')
							# cannot have `last` being a backslash anymore--it's considered consumed now
							last = c'\0'
							continue
						else: next = c # TODO: should probably be error: Invalid escape sequence
				else if c<>'\\'
					next = c
				if next is not nil
					chars.append(next)
				last = c
			return chars.toString

	def onSTRING_START_SINGLE(tok as IToken) as IToken
		require not _inSubstStringSingle
		_inSubstStringSingle = true
		tok.value = .tokValueForString(tok.text)
		_tokenDefsByWhich['STRING_PART_SINGLE'].isActive = true
		_tokenDefsByWhich['STRING_STOP_SINGLE'].isActive = true
		_tokenDefsByWhich['STRING_PART_FORMAT'].isActive = true
		return tok

	def onSTRING_PART_SINGLE(tok as IToken) as IToken
		require _inSubstStringSingle
		tok.value = .tokValueForString(tok.text)
		return tok

	def onSTRING_STOP_SINGLE(tok as IToken) as IToken
		require _inSubstStringSingle
		_inSubstStringSingle = false
		tok.value = .tokValueForString(tok.text)
		_tokenDefsByWhich['STRING_PART_SINGLE'].isActive = false
		_tokenDefsByWhich['STRING_STOP_SINGLE'].isActive = false
		_tokenDefsByWhich['STRING_PART_FORMAT'].isActive = false
		return tok


	def onSTRING_START_DOUBLE(tok as IToken) as IToken
		require not _inSubstStringDouble
		_inSubstStringDouble = true
		tok.value = .tokValueForString(tok.text)
		_tokenDefsByWhich['STRING_PART_DOUBLE'].isActive = true
		_tokenDefsByWhich['STRING_STOP_DOUBLE'].isActive = true
		_tokenDefsByWhich['STRING_PART_FORMAT'].isActive = true
		return tok

	def onSTRING_PART_DOUBLE(tok as IToken) as IToken
		require _inSubstStringDouble
		tok.value = .tokValueForString(tok.text)
		return tok

	def onSTRING_STOP_DOUBLE(tok as IToken) as IToken
		require _inSubstStringDouble
		_inSubstStringDouble = false
		tok.value = .tokValueForString(tok.text)
		_tokenDefsByWhich['STRING_PART_DOUBLE'].isActive = false
		_tokenDefsByWhich['STRING_STOP_DOUBLE'].isActive = false
		_tokenDefsByWhich['STRING_PART_FORMAT'].isActive = false
		return tok


	def onLBRACKET(tok as IToken) as IToken
		if _inSubstStringSingle or _inSubstStringDouble
			_substLBracketCount += 1
			if _substLBracketCount==1
				_tokenDefsByWhich['RBRACKET_SPECIAL'].isActive = true
				assert _tokenDefsByWhich['STRING_PART_FORMAT'].isActive
				_tokenDefsByWhich['STRING_PART_FORMAT'].isActive = false
		return tok

	def onRBRACKET_SPECIAL(tok as IToken) as IToken
		require
			_inSubstStringSingle or _inSubstStringDouble
			_substLBracketCount
		body
			_substLBracketCount -= 1
			if _substLBracketCount==0
				_tokenDefsByWhich['RBRACKET_SPECIAL'].isActive = false
				assert not _tokenDefsByWhich['STRING_PART_FORMAT'].isActive
				_tokenDefsByWhich['STRING_PART_FORMAT'].isActive = true
			tok.which = 'RBRACKET'  # tricky, tricky. the parser never sees an RBRACKET_SPECIAL
			return tok


	##
	## Doc Strings
	##

	def onDOC_STRING_START(tok as IToken) as IToken
		assert not _inDocString
		# narrow the tokenizer's token defs to a new shorter set
		# TODO: cache the tokens below
		.pushTokenDefs([
			TokenDef('DOC_STRING_STOP', r'[ \t]*"""[ \t]*\n'),
			TokenDef('DOC_STRING_BODY_TEXT', '.*\n'),
		])
		_inDocString = true
		return tok

	def onDOC_STRING_STOP(tok as IToken) as IToken
		assert _inDocString, tok
		_inDocString = false
		.popTokenDefs
		return tok

	def onDOC_STRING_BODY_TEXT(tok as IToken) as IToken
		assert _inDocString, tok
		return tok


	##
	## Simple string literals
	##

	def onSTRING_RAW_SINGLE(tok as IToken) as IToken
		require tok.text.startsWith('r')
		tok.value = tok.text.substring(2, tok.text.length-3)
		tok.which = 'STRING_SINGLE'
		return tok

	def onSTRING_RAW_DOUBLE(tok as IToken) as IToken
		require tok.text.startsWith('r')
		tok.value = tok.text.substring(2, tok.text.length-3)
		tok.which = 'STRING_DOUBLE'
		return tok

	def onSTRING_NOSUB_SINGLE(tok as IToken) as IToken
		require tok.text.startsWith('ns')
		tok.value = .tokValueForString(tok.text.substring(2))
		tok.which = 'STRING_SINGLE'
		return tok

	def onSTRING_NOSUB_DOUBLE(tok as IToken) as IToken
		require tok.text.startsWith('ns')
		tok.value = .tokValueForString(tok.text.substring(2))
		tok.which = 'STRING_DOUBLE'
		return tok

	def onSTRING_SINGLE(tok as IToken) as IToken
		tok.value = .tokValueForString(tok.text)
		return tok

	def onSTRING_DOUBLE(tok as IToken) as IToken
		tok.value = .tokValueForString(tok.text)
		return tok


	##
	## Deprecated
	##

	def onCUE(tok as IToken) as IToken
		assert false, tok  # wow, this doesn't even work
		tok.which = 'DEF'
		return tok
